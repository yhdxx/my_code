# run.py
import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
import random
import numpy as np
import pandas as pd
import os
from datetime import datetime

from model import ClientModel, ServerTail
from client import Client
from server import Server


def partition_dataset(dataset, num_clients, shards_per_client=2, seed=1234):
    """
    Non-IID partition: å°†æ•°æ®é›†æŒ‰æ ‡ç­¾åˆ† shardï¼Œæ¯ä¸ªå®¢æˆ·ç«¯æ‹¿è‹¥å¹²ä¸ª shard
    :param dataset: torchvision Dataset (MNIST)
    :param num_clients: å®¢æˆ·ç«¯æ•°é‡
    :param shards_per_client: æ¯ä¸ªå®¢æˆ·ç«¯åˆ†é…å¤šå°‘ä¸ªç±»åˆ« shard
    :param seed: éšæœºç§å­
    """
    np.random.seed(seed)
    labels = np.array(dataset.targets)  # MNIST çš„æ ‡ç­¾
    num_classes = labels.max() + 1

    # æŒ‰ç±»åˆ«å­˜ç´¢å¼•
    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]

    # æ‰“ä¹±æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬ç´¢å¼•
    for idxs in class_indices:
        np.random.shuffle(idxs)

    # å°†æ¯ä¸ªç±»åˆ«åˆ‡åˆ†ä¸ºå°ç‰‡ (shards)
    shards = []
    for c in range(num_classes):
        # è¿™é‡Œåˆ‡ shard çš„å¤§å°å¯ä»¥è°ƒæ•´ï¼Œç°åœ¨æŒ‰ (æ ·æœ¬æ•° // num_clients) åˆ‡
        split_size = max(1, len(class_indices[c]) // num_clients)
        shards.extend(np.array_split(class_indices[c], len(class_indices[c]) // split_size))

    np.random.shuffle(shards)

    # æ¯ä¸ªå®¢æˆ·ç«¯åˆ†é… shards_per_client ä¸ª shard
    parts = []
    shards_per_client_total = num_clients * shards_per_client
    assert shards_per_client_total <= len(shards), "shards æ•°é‡ä¸è¶³ï¼Œè¯·è°ƒå° shards_per_client"

    for i in range(num_clients):
        client_shards = shards[i * shards_per_client:(i + 1) * shards_per_client]
        client_indices = np.concatenate(client_shards, axis=0)
        parts.append(client_indices.tolist())

    return parts


def make_client_front_sizes(num_clients, tail_in_features):
    # create varied front architectures but same tail input dim
    out = []
    for i in range(num_clients):
        # front: input 784 -> hidden (random among options) -> tail_in_features
        h = random.choice([128, 256, 200])
        out.append([784, h, tail_in_features])
    return out


def main():
    torch.manual_seed(0)
    random_seed = 0
    device = 'cpu'  # change to 'cuda' if available
    num_clients = 4
    tail_in_features = 128  # cut size
    epochs = 100
    local_epochs = 1
    batch_size = 64

    # åˆ›å»ºç»“æœç›®å½•
    results_dir = "results"
    os.makedirs(results_dir, exist_ok=True)

    # åˆ›å»ºCSVæ–‡ä»¶è·¯å¾„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"federated_learning_results_{timestamp}.csv"
    csv_path = os.path.join(results_dir, csv_filename)

    # åˆå§‹åŒ–CSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
    df_columns = ['epoch']
    for i in range(num_clients):
        df_columns.extend([f'client_{i}_accuracy', f'client_{i}_loss'])
    df_columns.extend(['avg_accuracy', 'avg_loss'])

    # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼åˆå§‹åŒ–DataFrameï¼Œé¿å…è­¦å‘Š
    results_df = pd.DataFrame(columns=df_columns)
    # å…ˆå†™å…¥ä¸€ä¸ªç©ºè¡Œæ¥é¿å…FutureWarning
    results_df.loc[0] = [None] * len(df_columns)
    results_df.to_csv(csv_path, index=False)
    print(f"ç»“æœå°†ä¿å­˜åˆ°: {csv_path}")

    transform = transforms.Compose([transforms.ToTensor()])
    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

    # ğŸ”¹ ä½¿ç”¨ Non-IID æ•°æ®åˆ’åˆ†
    parts = partition_dataset(train_dataset, num_clients, shards_per_client=2)
    client_fronts = make_client_front_sizes(num_clients, tail_in_features)

    # Create server tail model
    server_tail = ServerTail(in_features=tail_in_features)
    server = Server(server_tail, lr=0.01, device=device)

    # Create clients
    clients = []
    client_loaders = []
    for i in range(num_clients):
        front_sizes = client_fronts[i]
        cm = ClientModel(front_sizes, tail_in_features=tail_in_features)
        client = Client(i, cm, device=device, lr_front=0.01)
        clients.append(client)
        subset = Subset(train_dataset, parts[i])
        loader = DataLoader(subset, batch_size=batch_size, shuffle=True)
        client_loaders.append(loader)

    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

    # é‡æ–°åˆå§‹åŒ–ç©ºçš„DataFrameç”¨äºæ”¶é›†ç»“æœ
    results_df = pd.DataFrame(columns=df_columns)

    # Main training loop
    for epoch in range(epochs):
        print(f"--- Global round {epoch + 1}/{epochs} ---")
        # Server distributes tail params
        tail_state = server.get_tail_state()
        for c in clients:
            c.set_tail_params(tail_state)
        # Each client trains locally and collects tail grads
        tail_grads_all = []
        for i, c in enumerate(clients):
            grads = c.train_local(client_loaders[i], epochs=local_epochs)
            if grads is None:
                # no grads? skip
                continue
            tail_grads_all.append(grads)
            print(f"Client {i} uploaded tail grads (shapes): {list(grads.keys())}")

        # Server aggregates and updates tail
        server.aggregate_and_update(tail_grads_all)

        # Evaluate clients (on local model with updated tail)
        accs = []
        losses = []
        epoch_results = {'epoch': epoch + 1}

        for i, c in enumerate(clients):
            # make sure client has latest tail
            c.set_tail_params(server.get_tail_state())
            loss, acc = c.evaluate(test_loader)
            accs.append(acc)
            losses.append(loss)
            epoch_results[f'client_{i}_accuracy'] = acc
            epoch_results[f'client_{i}_loss'] = loss

        avg_acc = np.mean(accs)
        avg_loss = np.mean(losses)
        epoch_results['avg_accuracy'] = avg_acc
        epoch_results['avg_loss'] = avg_loss

        print(f"Eval across clients (avg acc): {avg_acc:.4f}, avg loss: {avg_loss:.4f}")

        # å°†æœ¬è½®ç»“æœæ·»åŠ åˆ°DataFrameå¹¶ä¿å­˜åˆ°CSV
        new_row = pd.DataFrame([epoch_results])
        if results_df.empty:
            results_df = new_row
        else:
            results_df = pd.concat([results_df, new_row], ignore_index=True)

        results_df.to_csv(csv_path, index=False)

    print(f"è®­ç»ƒå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")

    # æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    print("\n=== è®­ç»ƒç»“æœç»Ÿè®¡ ===")
    print(f"æœ€ç»ˆå¹³å‡å‡†ç¡®ç‡: {avg_acc:.4f}")
    print(f"æœ€ç»ˆå¹³å‡æŸå¤±: {avg_loss:.4f}")
    if not results_df.empty:
        max_acc_epoch = results_df['avg_accuracy'].idxmax() + 1
        max_acc = results_df['avg_accuracy'].max()
        print(f"æœ€é«˜å¹³å‡å‡†ç¡®ç‡: {max_acc:.4f} (ç¬¬ {max_acc_epoch} è½®)")

    return csv_path


if __name__ == "__main__":
    csv_file = main()
